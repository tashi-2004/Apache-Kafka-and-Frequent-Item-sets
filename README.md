# Apache-Kafka-and-Frequent-Item-sets

# Overview
This project focuses on implementing a streaming pipeline for processing and analyzing the Amazon Metadata dataset using various techniques such as sampling, preprocessing, frequent itemset mining, and database integration.

# Files
sampling.py: Python script for sampling the Amazon Metadata dataset.
pre-processing.py: Python script for preprocessing the sampled dataset.
producer.py: Python script for the producer application in the streaming pipeline setup.
consumer1.py: Python script for the first consumer application subscribing to the producer's data stream.
consumer2.py: Python script for the second consumer application subscribing to the producer's data stream.
consumer3.py: Python script for the third consumer application subscribing to the producer's data stream.
Apriori.py: Python script implementing the Apriori algorithm for frequent itemset mining.
PCY.py: Python script implementing the PCY algorithm for frequent itemset mining.
Bloomfilter.py: Python script for implementing the Bloom Filter data structure.
database_apriori,PCY,BloomFilter.py: Python script for integrating with the MongoDB Compass database.

# Description
# Sampling and Preprocessing: 
The sampling.py script is used to sample the Amazon Metadata dataset, followed by preprocessing using pre-processing.py.
# Streaming Pipeline Setup: 
The producer.py script generates a data stream, while consumer1.py, consumer2.py, and consumer3.py subscribe to this stream to perform various tasks.
# Frequent Itemset Mining:
Both Apriori.py and PCY.py scripts implement different algorithms for frequent itemset mining, while Bloomfilter.py provides support for efficient data processing.
# Database Integration:
The database_integration.py script connects to a MongoDB compass database and stores the results of the analysis.
# MongoDB Compass
The output generated by  database_apriori,PCY,BloomFilter.py can be viewed in MongoDB Compass after integration with the MongoDB database.

# Usage
Run sampling.py followed by pre-processing.py to sample and preprocess the dataset.
Run producer.py to start the data stream.
Run consumer1.py, consumer2.py, and consumer3.py to subscribe to the data stream and perform analysis.
Optionally, run Apriori.py, PCY.py, and Bloomfilter.py for additional analysis.
Run database_integration.py to integrate with the MongoDB database.

# Requirements
Python 3.x
Kafka
MongoDB Compass & MongoDB Connector
Other dependencies as specified in the codes
